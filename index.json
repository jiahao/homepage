[{"authors":["Jiahao Chen"],"categories":null,"content":"","date":1545145745,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545146550,"objectID":"7ca2b85716c4b8994944e2fa781611c2","permalink":"https://jiahao.github.io/talk/2019-cornell-cam/","publishdate":"2018-12-18T10:09:05-05:00","relpermalink":"/talk/2019-cornell-cam/","section":"talk","summary":"","tags":[],"title":"2019 Cornell CAM Colloquium","type":"talk"},{"authors":null,"categories":null,"content":"I am a core contributor to the Julia programming language. In addition to starting and running the Julia Lab at MIT CSAIL from 2013\u0026mdash;2017, I have also made over 500 commits to the core Julia repository, and authored over 80 Julia packages spread over the JuliaMatrices, JuliaMath, JuliaParallel, JuliaQuantum, JuliaSparse, JuliaStats, and JuliaText organizations, and also my personal account. I am also involved in the JuliaCN documentation translation efforts into Mandarin Chinese, the JuliaCon organizing committee (including chairing the first three JuliaCons 2014\u0026mdash;2016), and broader outreach efforts through the NumFOCUS nonprofit.\n","date":1545109446,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545146550,"objectID":"21c07d8624497a6d97f3baa4e7183c57","permalink":"https://jiahao.github.io/project/julia/","publishdate":"2018-12-18T00:04:06-05:00","relpermalink":"/project/julia/","section":"project","summary":"The Julia programming language","tags":["julia","machine-learning","data-science"],"title":"Julia","type":"project"},{"authors":["Jiahao Chen","Nathan Kallus","Xiaojie Mao","Geoffry Svacha","Madeleine Udell"],"categories":null,"content":"","date":1543381200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545146550,"objectID":"0d215271a0e226c60ae93aeb6a35f81d","permalink":"https://jiahao.github.io/publication/chen-2019-fatstar/","publishdate":"2018-11-28T00:00:00-05:00","relpermalink":"/publication/chen-2019-fatstar/","section":"publication","summary":"Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.","tags":null,"title":"Fairness under unawareness: disparate impact evaluation with proxy for unknown protected attribute","type":"publication"},{"authors":["Jiahao Chen","Nathan Kallus","Xiaojie Mao","Geoffry Svacha","Madeleine Udell"],"categories":null,"content":"My intern, Xiaojie Mao, will be presenting his summer research work.\n","date":1542552975,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545196173,"objectID":"cd99cfecebc73588754700e152fd4fb6","permalink":"https://jiahao.github.io/talk/2019-fatstar/","publishdate":"2018-11-18T09:56:15-05:00","relpermalink":"/talk/2019-fatstar/","section":"talk","summary":"My intern, Xiaojie Mao, will be presenting his summer research work.","tags":[],"title":"Fairness under unawareness: assessing disparity when protected class is unobserved","type":"talk"},{"authors":["Manuela Veloso","Nathan Kallus","Senthil Kumar","Sameena Shah","Isabelle Moulinier","John Paisley","Jiahao Chen"],"categories":null,"content":"Pleased to announce that I am organizing the First NeurIPS Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy.\n","date":1539924845,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545196173,"objectID":"5813418b27a446117fb62a5b5eabdf6e","permalink":"https://jiahao.github.io/talk/2018-ai4fin/","publishdate":"2018-10-18T23:54:05-05:00","relpermalink":"/talk/2018-ai4fin/","section":"talk","summary":"Pleased to announce that I am organizing the First NeurIPS Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy.","tags":[],"title":"2018 NeurIPS Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy (FEAP-AI4Fin)","type":"talk"},{"authors":["Jiahao Chen"],"categories":[],"content":"I\u0026rsquo;m hiring a team at Capital One in New York for R\u0026amp;D efforts to bring state-of-the-art machine learning techniques to the retail financial services industry. Capital One’s goal is to Change Banking for Good, and in this position, you will be in a very real position to transform entire business processes by:\n driving cutting-edge research in the rapidly growing area of FAT/FATE/FATES ML (fair, accountable, transparent, ethical, safe, and secure machine learning), connecting the academic machine learning community to real and tangible challenges in the banking and financial services industry, advocating for fair and explainable machine learning within the business, and putting new tests, algorithms and processes into production together with business partners across the company.  What my team has been up to:\n In October, I presented short position paper on why banks need fair and explainable machine learning at FATREC (a ACM RecSys workshop). Fairness and explainability are not only nice to have, but in many applications in banking, they are actually required by law!\n In December, we will co-organize the NIPS 2018 Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy, together with industry partners at JPMorgan and S\u0026amp;P Global, and academic partners at Columbia, Cornell, and CMU.\n In January, we will present a paper jointly authored with Cornell University at the ACM FAT* conference,on statistical biases inherent to tests for inequity with unknown labels\n  I\u0026rsquo;m seeking machine learning researchers who are interested in digging deep into the banking business and finding opportunities to articulate and solve new research problems that surface from our concrete and practical business needs.\nPlease see our official job listing on Workday. We’re hiring at the Principal Associate level (this posting) and also at the more experienced Senior Manager level (PhD + relevant industry experience).\n","date":1539578809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545195143,"objectID":"56c001135eb273a4602aacde50371c5b","permalink":"https://jiahao.github.io/post/2018-11-hiring/","publishdate":"2018-10-14T23:46:49-05:00","relpermalink":"/post/2018-11-hiring/","section":"post","summary":"I\u0026rsquo;m hiring a team at Capital One in New York for R\u0026amp;D efforts to bring state-of-the-art machine learning techniques to the retail financial services industry. Capital One’s goal is to Change Banking for Good, and in this position, you will be in a very real position to transform entire business processes by:\n driving cutting-edge research in the rapidly growing area of FAT/FATE/FATES ML (fair, accountable, transparent, ethical, safe, and secure machine learning), connecting the academic machine learning community to real and tangible challenges in the banking and financial services industry, advocating for fair and explainable machine learning within the business, and putting new tests, algorithms and processes into production together with business partners across the company.","tags":[],"title":"Hiring ML Researchers at Capital One","type":"post"},{"authors":["Jiahao Chen"],"categories":null,"content":"","date":1538838156,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545196173,"objectID":"2cd5f888cc8fc7a32f8e731e523aadd3","permalink":"https://jiahao.github.io/talk/2018-fatrec/","publishdate":"2018-10-06T10:02:36-05:00","relpermalink":"/talk/2018-fatrec/","section":"talk","summary":"","tags":[],"title":"Fair Lending Needs Explainable Models for Responsible Recommendation","type":"talk"},{"authors":["Jiahao Chen"],"categories":null,"content":"","date":1536724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545146550,"objectID":"1bf599f8c2eb94d523c3a76d08fd8c06","permalink":"https://jiahao.github.io/publication/chen-2018-fatrec/","publishdate":"2018-09-12T00:00:00-04:00","relpermalink":"/publication/chen-2018-fatrec/","section":"publication","summary":"The financial services industry has unique explainability and fairness challenges arising from compliance and ethical considerations in credit decisioning. These challenges complicate the use of model machine learning and artificial intelligence methods in business decision processes.","tags":null,"title":"Fair lending needs explainable models for responsible recommendation","type":"publication"}]